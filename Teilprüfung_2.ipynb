{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0812\n",
      "Epoch 100, Loss: 0.0533\n",
      "Epoch 200, Loss: 0.0522\n",
      "Epoch 300, Loss: 0.0511\n",
      "Epoch 400, Loss: 0.0500\n",
      "Epoch 500, Loss: 0.0490\n",
      "Epoch 600, Loss: 0.0483\n",
      "Epoch 700, Loss: 0.0476\n",
      "Epoch 800, Loss: 0.0471\n",
      "Epoch 900, Loss: 0.0467\n",
      "Trainingsgenauigkeit: 93.75%\n",
      "Testgenauigkeit: 95.00%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Dateiname: Teilprüfung_2\n",
    "# Die Aufgabe besteht aus 2 Teilen:\n",
    "#   * Theoretischer Teil: \n",
    "#     Erklärung, wie die Einführung der bedingten Korrelation und die Verwendung\n",
    "#     der ReLU-Funktion (Rectified Linear Unit) die Fähigkeit eines tiefen neuronalen Netzes\n",
    "#     verbessert, komplexe Muster zu erkennen. Beispiel: ein neuronales Netz trainiert wird,\n",
    "#     um zwischen Bildern von Katzen und Hunden zu unterscheiden.\n",
    "#   * Praktischer Teil:\n",
    "#     Python-Skript, das die Initialisierung eines kleinen neuronalen Netzes mit einer \n",
    "#     versteckten Schicht zeigt, die ReLU-Funktion anwendet und die bedingte Korrelation illustriert.  \n",
    "# Autor: Ksenia Meks\n",
    "# Letzte Änderung: 14.02.2025\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "'''  \n",
    "Bedingte Korrelation durch ReLU - die Logik-Erklärung:\n",
    "\n",
    "Wenn ein einfaches lineares 2-schichtiges neuronales Netz trainieren wollte, \n",
    "anhand der Pixelwerte vorherzusagen, ob auf einem Bild eine Katze zu sehen ist, \n",
    "hätte es ein Problem: ein einzelnes Pixel korreliert nicht damit, ob das Bild eine Katze zeigt, \n",
    "sondern nur verschiedene Pixelanordnungen.\n",
    "Das bedeutet, dass es nur einfache Muster wie „Ist das Bild heller oder dunkler?“ \n",
    "oder „Ist das Tier eher links oder rechts im Bild?“ erfassen könnte.\n",
    "Ein neuronales Netz ohne Nichtlinearitäten (also nur mit linearen Aktivierungsfunktionen wie f(x) = x)\n",
    "könnte nur lineare Zusammenhänge zwischen Eingaben und Ausgaben lernen.\n",
    "\n",
    "Die Unterscheidung zwischen Katzen und Hunden ist aber ein hochkomplexes Problem, \n",
    "das viele nichtlineare Merkmale umfasst, wie Ohrenformen, Felltexturen oder Augenformen. \n",
    "Eine lineare Transformation könnte diese Merkmale nicht gut trennen.\n",
    "\n",
    "Deshalb werden zwischenliegende Schichten (Datenmengen) des Netztes erzeugt, in denen die Knoten \n",
    "das Vorhandensein oder das Fehlen einer unterschiedlichen Konfiguration von Eingaben repräsentieren.\n",
    "Auf diese Weise muss bei der Datenmenge der Katzenbilder ein einzelnes Pixel nicht damit korrelieren, \n",
    "ob auf einem Bild eine Katze zu sehen ist, aber stattdessen versucht die mittlere Schicht, verschiedene \n",
    "Pixelanordnungen zu identifizieren, die mit dem Vorhandensein einer Katze korrelieren oder nicht.\n",
    "Beispielweise: das Vorhandensein vieler katzenartiger Pixelanordnungen liefert der letzten Schicht \n",
    "die Informationen (die Korrelation), die sie benötigt, um das Vorhandensein oder das Fehlen \n",
    "einer Katze richtig vorherzusagen.\n",
    "\n",
    "In diesem Fall ist es erforderlich, dass diese mittlere Schicht in der Lage ist, selektiv mit der \n",
    "Eingabe zu korrelieren, d.h.: \n",
    "    die mittlere Schicht soll manchmal mit einer Eingabe korrelieren, manchmal aber auch nicht. \n",
    "Das verleiht der mittleren Schicht eine besondere Art der Korrelation: die bedingte Korrelation.\n",
    "\n",
    "Was dem 3-Schichtigen (zumindest) Netzt ermöglicht, bei Bedarf mit verschiedenen Eingaben zu korrelieren\n",
    "ist das Deaktivieren eines Knotens der mittleren Schicht, wenn der Wert negativ ist.\n",
    "Diese Logik, den Wert des Knotens auf 0 zu setzen, wenn er negativ ist, heißt Nichtlinearität. \n",
    "Ohne diese Optimierung ist das neuronale Netz linear.\n",
    "\n",
    "Es gibt viele verschiedene Arten von Nichtlinearitäten und eine von denen ist ReLu.\n",
    "\n",
    "ReLu (Rectified Linear Unit) ist eine von Aktivierungsfunktionen (d.h. Funktionen, die auf die Neuronen\n",
    "in einer Schicht während der Vorhersage angewendet werden) und sie bewirkt genau f(x) = max(0, x)\n",
    "Durch die ReLU-Funktion entstehen regionale Aktivierungen im neuronalen Netz, d. h.: \n",
    "    das Netzwerk kann lernen, bestimmte Merkmale nur unter bestimmten Bedingungen zu aktivieren.\n",
    "Beispiel mit Katzen und Hunden:\n",
    "*  Eine bestimmte Gruppe von Neuronen könnte lernen, spitze Ohren zu aktivieren \n",
    "        → Diese Aktivierung wird durch ReLU auf 0 gesetzt, wenn keine spitzen Ohren erkannt wurden.\n",
    "*  Eine andere Gruppe könnte lernen, hängende Ohren zu aktivieren \n",
    "        → Diese Aktivierung wird durch ReLU auf 0 gesetzt, wenn keine hängende Ohren erkannt wurden.\n",
    "So entstehen bedingte Korrelationen durch ReLu: \n",
    "    Das Netzwerk erkennt nicht nur „irgendwelche Ohren“, sondern spezifisch:\n",
    "        „spitze Ohren → eher Katze“ oder „hängende Ohren → eher Hund“.\n",
    "        \n",
    "Zusammenfassung:\n",
    "->  Ohne Nichtlinearitäten wie ReLU könnte ein neuronales Netz nur lineare Muster erkennen, \n",
    "    was für Katzen/Hunde nicht ausreicht.\n",
    "->  ReLU macht das Modell nichtlinear und lässt es komplexe Muster erfassen.\n",
    "->  Die bedingte Korrelation entsteht, weil nur bestimmte Neuronen unter bestimmten Bedingungen \n",
    "    aktiviert werden.\n",
    "->  Das Beispiel in Python zeigt, wie ReLU Werte filtert und dadurch die Fähigkeit des Netzwerks \n",
    "    verbessert.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # Für reproduzierbare Ergebnisse\n",
    "\n",
    "# DATEN -> 1000 Messungen mit Zufallsfunktion simuliert\n",
    "\n",
    "#    Zufällige Generierung von Merkmalen für Katzen und Hunde\n",
    "spitze_ohren = np.random.uniform(-5, 5, 1000)    # Katzen eher hohe Werte, Hunde niedrig\n",
    "haengende_ohren = np.random.uniform(-5, 5, 1000) # Hunde hohe Werte, Katzen niedrig\n",
    "felltextur = np.random.uniform(0, 10, 1000)      # Weiches Fell (Katze) hohe Werte vs. hartes Fell (Hund)\n",
    "augenform = np.random.uniform(1, 5, 1000)        # Mandelförmig (Katze) niedrige Werte vs. Rund (Hund)\n",
    "\n",
    "# MATRIX -> Zusammenführen der Daten in eine Matrix \n",
    "daten_matrix = np.column_stack((spitze_ohren, haengende_ohren, felltextur, augenform))\n",
    "\n",
    "# DATEN VORBEREITUNG -> geplante Normalisieren und Aufteilen, Labels\n",
    "\n",
    "#    Normalisieren der Sensordaten mithilfe der Min-Max-Normalisierung\n",
    "min_vals = np.min(daten_matrix, axis=0)\n",
    "max_vals = np.max(daten_matrix, axis=0)\n",
    "daten_matrix = (daten_matrix - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "#    Labels generieren (Wahrscheinlichkeit, ob ein Tier eher eine Katze (1) oder ein Hund (0) ist)\n",
    "#           Spitze Ohren hoch, Hängende Ohren niedrig, Felltextur hoch (weiches Fell), Augenform niedrig -> eher Katze\n",
    "y_labels =  ((spitze_ohren > 0) & (haengende_ohren < 0) & (felltextur > 5) & (augenform < 3)).astype(int).reshape(-1, 1)\n",
    "\n",
    "#    Aufteilen der Daten in 80% Trainings- und 20% Testdatensatz\n",
    "train_size = int(0.8 * len(daten_matrix))\n",
    "X_train, X_test = daten_matrix[:train_size], daten_matrix[train_size:]\n",
    "y_train, y_test = y_labels[:train_size], y_labels[train_size:]\n",
    "\n",
    "# INITIALISIERUN -> Klasse für das neuronale Netz\n",
    "\n",
    "class NeuronalesNetz:\n",
    "    def __init__(self, input_size, hidden_size, output_size, alpha=0.05):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Gewichte und Biases initialisieren\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.2\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.2\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Erste Schicht (Eingabe -> versteckte Schicht)\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)    # ReLu nur für die mittlere Schicht (übliche Vorgehensweise, \n",
    "                                        #   besonders im Fall einer binären Klassifikation -> Ausgabe im Bereich [0,1])\n",
    "                                        # ReLu hilf hier Nichtlinearität zu modellieren\n",
    "                                        # Es hilt übermäßige Sättigung zu vermeiden und Lernfähigkeit zu erühen\n",
    "        \n",
    "        # Zweite Schicht (versteckte Schicht -> Ausgabe)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.z2\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]  # Anzahl der Trainingsbeispiele\n",
    "        output = self.forward(X)  # Vorhersage durch das Netz\n",
    "        error = output - y  # Fehlerberechnung\n",
    "        \n",
    "        # Fehler im Output\n",
    "        dz2 = error\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Fehler in der versteckten Schicht (Backprop durch ReLU)\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.relu_derivative(self.z1)  \n",
    "                                       # ReLu sorgt dafür, dass nur Fehler, \n",
    "                                       # die von aktiven Neuronen in der versteckten Schicht kommen, \n",
    "                                       # weitergegeben werden -> Lernen-Effizienz\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Gewichte aktualisieren\n",
    "        self.W2 -= self.alpha * dW2\n",
    "        self.b2 -= self.alpha * db2\n",
    "        self.W1 -= self.alpha * dW1\n",
    "        self.b1 -= self.alpha * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for i in range(epochs):\n",
    "            self.backward(X, y)\n",
    "            if i % 100 == 0:\n",
    "                loss = np.mean((self.forward(X) - y) ** 2)\n",
    "                print(f'Epoch {i}, Loss: {loss:.4f}')\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        predictions = self.forward(X) >= 0.5\n",
    "        return np.mean(predictions == y) \n",
    "\n",
    "# ARCHITEKTUR -> geplante Schichten & Neuronen des tiefen neuronalen Netzes\n",
    "\n",
    "#    Eingabeschicht:      4 Neuronen: 4 Merkmale hast (Spitze Ohren, Hängende Ohren, Felltextur, Augenform)\n",
    "input_size = 4 \n",
    "#    Versteckte Schicht:  8 Neuronen: ein Hyperparameter, den man testen muss. Ich habe die doppelte Anzahl der Eingaben genommen um mehr Kapatzität zu haben\n",
    "hidden_size = 8\n",
    "#    Ausgabeschicht:      1 Neuron: Eine binäre Klassifikation (Katze 1, Hund 0) hat normalerweise nur ein einziges Neuron\n",
    "output_size = 1\n",
    "\n",
    "# MODELL -> Modell erstellen und trainieren\n",
    "\n",
    "#    Netz initialisieren\n",
    "netz = NeuronalesNetz(input_size, hidden_size, output_size)\n",
    "#    Training\n",
    "netz.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "# ERGEBNISSE -> Ausgabe\n",
    "\n",
    "print(f'Trainingsgenauigkeit: {netz.accuracy(X_train, y_train) * 100:.2f}%')\n",
    "print(f'Testgenauigkeit: {netz.accuracy(X_test, y_test) * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
