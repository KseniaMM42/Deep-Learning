{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2499\n",
      "Epoch 100, Loss: 0.2419\n",
      "Epoch 200, Loss: 0.2349\n",
      "Epoch 300, Loss: 0.2287\n",
      "Epoch 400, Loss: 0.2233\n",
      "Epoch 500, Loss: 0.2185\n",
      "Epoch 600, Loss: 0.2144\n",
      "Epoch 700, Loss: 0.2108\n",
      "Epoch 800, Loss: 0.2076\n",
      "Epoch 900, Loss: 0.2048\n",
      "Trainingsgenauigkeit: 76.12%\n",
      "Testgenauigkeit: 77.50%\n"
     ]
    }
   ],
   "source": [
    "# 6.15.C.01\n",
    "\n",
    "# Aufgabe: Überanpassung, Backpropagation\n",
    "# Die Aufgabe ist es, ein tiefes neuronales Netz zu entwickeln, das die Wahrscheinlichkeit \n",
    "#     eines Unfalls anhand von verschiedenen Sensordaten eines Fahrzeugs vorhersagt.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # Für reproduzierbare Ergebnisse\n",
    "\n",
    "# DATEN -> 1000 Messungen mit Zufallsfunktion simuliert - aber in realistischen Bereichen\n",
    "\n",
    "#    Geschwindigkeit: 0 bis 200 km/h \n",
    "geschwindigkeit = np.random.uniform(0, 200, 1000) \n",
    "#    Beschleunigung: -3 m/s² (Bremsen) bis 3 m/s² (Beschleunigen) \n",
    "beschleunigung = np.random.uniform(-3, 3, 1000) \n",
    "#    Bremsverhalten: 0 (nicht bremsend) oder 1 (bremsend) \n",
    "bremsverhalten = np.random.randint(0, 2, 1000) \n",
    "#    Lenkwinkel: -45 Grad (links) bis 45 Grad (rechts) \n",
    "lenkwinkel = np.random.uniform(-45, 45, 1000) \n",
    "\n",
    "# MATRIX -> Zusammenführen der Daten in eine Matrix \n",
    "\n",
    "daten_matrix = np.column_stack((geschwindigkeit, beschleunigung, bremsverhalten, lenkwinkel)) \n",
    "\n",
    "# DATEN VORBEREITUNG -> geplante Normalisieren und Aufteilen, Labels\n",
    "\n",
    "#    Normalisieren der Sensordaten mithilfe der Min-Max-Normalisierung\n",
    "min_vals = np.min(daten_matrix, axis=0)\n",
    "max_vals = np.max(daten_matrix, axis=0)\n",
    "daten_matrix = (daten_matrix - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "#    Labels generieren (Unfallwahrscheinlichkeit basierend auf bestimmten Bedingungen)\n",
    "#           Labels, bei denen die Unfallwahrscheinlichkeit 1 ist, wenn die Geschwindigkeit größer als 100 km/h ist und das Bremsverhalten aktiv ist (d.h. bremsverhalten == 1)\n",
    "y_labels = ((geschwindigkeit > 100) & (bremsverhalten == 1)).astype(int).reshape(-1, 1)\n",
    "\n",
    "#    Aufteilen der Daten in 80% Trainings- und 20% Testdatensatz\n",
    "train_size = int(0.8 * len(daten_matrix))\n",
    "X_train, X_test = daten_matrix[:train_size], daten_matrix[train_size:]\n",
    "y_train, y_test = y_labels[:train_size], y_labels[train_size:]\n",
    "\n",
    "# BACKPROPAGATION\n",
    "\n",
    "class NeuronalesNetz:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, lr=0.01, l2_lambda=0.0001):\n",
    "        self.lr = lr\n",
    "        self.l2_lambda = l2_lambda\n",
    "        \n",
    "        # Initialisierung der Gewichte und Biases für jede Schicht\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "        self.W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    ''' \n",
    "    # diese funktion ist hier nicht benötigt !\n",
    "    def sigmoid_derivative(self, x):  # ??????????????\n",
    "        return x * (1 - x)\n",
    "    '''\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        # # Vorwärtsdurchlauf (Feedforward)\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.relu(self.Z1)    # Erste versteckte Schicht\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.relu(self.Z2)    # Zweite versteckte Schicht\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = self.sigmoid(self.Z3)  # Ausgabeschicht \n",
    "        return self.A3\n",
    "    \n",
    "    def backpropagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        output = self.feedforward(X)\n",
    "        error = output - y\n",
    "        \n",
    "        # Fehler und Gradientenberechnung für jede Schicht\n",
    "        dZ3 = error * (output * (1 - output))\n",
    "        dW3 = np.dot(self.A2.T, dZ3) / m + self.l2_lambda * self.W3\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dZ2 = np.dot(dZ3, self.W3.T) * self.relu_derivative(self.A2)\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m + self.l2_lambda * self.W2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * self.relu_derivative(self.A1)\n",
    "        dW1 = np.dot(X.T, dZ1) / m + self.l2_lambda * self.W1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update der Gewichte\n",
    "        self.W3 -= self.lr * dW3\n",
    "        self.b3 -= self.lr * db3\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2 \n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for i in range(epochs):\n",
    "            self.backpropagation(X, y)\n",
    "            if i % 100 == 0:\n",
    "                loss = np.mean((self.feedforward(X) - y) ** 2)\n",
    "                print(f'Epoch {i}, Loss: {loss:.4f}')\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        predictions = self.feedforward(X) >= 0.5\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "# ARCHITEKTUR -> geplante Schichten & Neuronen des tiefen neuronalen Netzes\n",
    "\n",
    "#    Eingabeschicht:        4 Neuronen (für jede Sensordatenkategorie eines)\n",
    "input_size = 4 \n",
    "#    Versteckte Schicht 1: 64 Neuronen, Aktivierungsfunktion: ReLU \n",
    "hidden_size1 = 64\n",
    "#    Versteckte Schicht 2: 32 Neuronen, Aktivierungsfunktion: ReLU \n",
    "hidden_size2 = 32\n",
    "#    Ausgabeschicht:        1 Neuron,   Aktivierungsfunktion: Sigmoid (für die Unfallwahrscheinlichkeit)\n",
    "output_size = 1\n",
    "\n",
    "# MODELL -> Modell erstellen und trainieren\n",
    "\n",
    "#    Modell erstellen\n",
    "netz = NeuronalesNetz(input_size, hidden_size1, hidden_size2, output_size, lr=0.01, l2_lambda=0.0001)\n",
    "#    Training\n",
    "netz.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "# ERGEBNISSE -> Ausgabe\n",
    "\n",
    "print(f'Trainingsgenauigkeit: {netz.accuracy(X_train, y_train) * 100:.2f}%')\n",
    "print(f'Testgenauigkeit: {netz.accuracy(X_test, y_test) * 100:.2f}%')\n",
    "\n",
    "# ERKLÄRUNG -> Antwort auf die Frage \"Fehlerattributierung & Überanpassung\"\n",
    "#\n",
    "# Frage:\n",
    "# Erkläre, wie du die Fehlerattribuierung durchführst, um das Netzwerk vor Überanpassung \n",
    "# zu schützen und indirekte Korrelationen zu lernen. \n",
    "# \n",
    "# Antwort:\n",
    "# Das neuronale Netz verwendet Backpropagation, um Fehler zu attribuieren und Gewichte anzupassen.\n",
    "# Der Hauptmechanismus zur Vermeidung von Überanpassung (Overfitting) sind:\n",
    "#     1) L2-Regularisierung (Gewichtsstrafe): \n",
    "#        Bei der Berechnung der Gradienten wird eine zusätzliche Komponente hinzugefügt: + self.l2_lambda * W\n",
    "#        Dadurch werden große Gewichtswerte bestraft und das Netz bleibt „glatt“.\n",
    "#     2) sigmoid- und ReLU-Aktivierungsfunktionen, um den Gradientenfluss zu steuern.\n",
    "#     3) Indirekte Korrelationen durch tiefe Schichten:\n",
    "#        2 versteckten Schichten (64 und 32 Neuronen) lernen nichtlineare Kombinationen der Eingangsdaten.\n",
    "#        ReLU hilft dabei, nicht-lineare Zusammenhänge zu erfassen.\n",
    "#        Sigmoid-Aktivierung in der Ausgabeschicht zwingt das Modell, eine Wahrscheinlichkeitsverteilung \n",
    "#             für Unfallrisiken zu lernen.\n",
    "#     4) Verlust (Loss) wird nach 100 Epochen ausgegeben, um die Netzwerkanpassung zu überwachen.\n",
    "#     5) Nach dem Training wird die Testgenauigkeit berechnet:\n",
    "#        Falls sie stark von der Trainingsgenauigkeit abweicht, liegt Overfitting vor.\n",
    "# Zusammenfassung:\n",
    "#     Die Kombination aus L2-Regularisierung, Aktivierungsfunktionen, tiefen Schichten und \n",
    "#     iterativem Verlust-Tracking ermöglicht es dem Modell, indirekte Korrelationen zu lernen,\n",
    "#     ohne zu sehr an die Trainingsdaten angepasst zu werden. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
